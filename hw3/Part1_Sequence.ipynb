{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xqrWupU2ESJ"
   },
   "source": [
    "$$\n",
    "\\newcommand{\\mat}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\mattr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\matinv}[1]{\\boldsymbol {#1}^{-1}}\n",
    "\\newcommand{\\vec}[1]{\\boldsymbol {#1}}\n",
    "\\newcommand{\\vectr}[1]{\\boldsymbol {#1}^\\top}\n",
    "\\newcommand{\\rvar}[1]{\\mathrm {#1}}\n",
    "\\newcommand{\\rvec}[1]{\\boldsymbol{\\mathrm{#1}}}\n",
    "\\newcommand{\\diag}{\\mathop{\\mathrm {diag}}}\n",
    "\\newcommand{\\set}[1]{\\mathbb {#1}}\n",
    "\\newcommand{\\norm}[1]{\\left\\lVert#1\\right\\rVert}\n",
    "\\newcommand{\\pderiv}[2]{\\frac{\\partial #1}{\\partial #2}}\n",
    "\\newcommand{\\bb}[1]{\\boldsymbol{#1}}\n",
    "$$\n",
    "# Part 1: Sequence Models\n",
    "<a id=part1></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1640375368123,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "-mNwk59Y2EzD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18283,
     "status": "ok",
     "timestamp": 1640375386778,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "laJ3WDtY2F54",
    "outputId": "5a0f605b-9945-4051-f2b6-ce215089486e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1059,
     "status": "ok",
     "timestamp": 1640375387831,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "bykEfCrs2Jmy",
    "outputId": "6a542cc1-f9d8-4104-ada3-2ac0931ca055"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8IotZe02ESN"
   },
   "source": [
    "In this part we will learn about working with text sequences using recurrent neural networks.\n",
    "We'll go from a raw text file all the way to a fully trained GRU-RNN model and generate works of art!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T22:07:01.595926Z",
     "iopub.status.busy": "2021-12-26T22:07:01.594929Z",
     "iopub.status.idle": "2021-12-26T22:07:03.014447Z",
     "shell.execute_reply": "2021-12-26T22:07:03.014447Z"
    },
    "executionInfo": {
     "elapsed": 5964,
     "status": "ok",
     "timestamp": 1640375393794,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "fXHpszCM2ESO",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import urllib\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-26T22:07:03.019433Z",
     "iopub.status.busy": "2021-12-26T22:07:03.018437Z",
     "iopub.status.idle": "2021-12-26T22:07:03.144099Z",
     "shell.execute_reply": "2021-12-26T22:07:03.145097Z"
    },
    "executionInfo": {
     "elapsed": 628,
     "status": "ok",
     "timestamp": 1640375394418,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "bGWpsaxZ2ESQ",
    "outputId": "3884afef-b6e2-4f82-afa1-e2f135b1667c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "test = unittest.TestCase()\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9r8rp2W2ESR"
   },
   "source": [
    "## Text generation with a char-level RNN\n",
    "<a id=part1_1></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHVVX4Dc2ESR"
   },
   "source": [
    "### Obtaining the corpus\n",
    "<a id=part1_2></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqCwpZkY2ESS"
   },
   "source": [
    "Let's begin by downloading a corpus containing all the works of William Shakespeare.\n",
    "Since he was very prolific, this corpus is fairly large and will provide us with enough data for\n",
    "obtaining impressive results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-26T22:07:03.150105Z",
     "iopub.status.busy": "2021-12-26T22:07:03.149085Z",
     "iopub.status.idle": "2021-12-26T22:07:03.271758Z",
     "shell.execute_reply": "2021-12-26T22:07:03.271758Z"
    },
    "executionInfo": {
     "elapsed": 486,
     "status": "ok",
     "timestamp": 1640375394899,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "y47xJjiZ2EST",
    "outputId": "1c243d13-0e68-4f68-d491-a60822d63083",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus file C:\\Users\\x3yusk\\.pytorch-datasets\\shakespeare.txt exists, skipping download.\n"
     ]
    }
   ],
   "source": [
    "CORPUS_URL = 'https://github.com/cedricdeboom/character-level-rnn-datasets/raw/master/datasets/shakespeare.txt'\n",
    "DATA_DIR = pathlib.Path.home().joinpath('.pytorch-datasets')\n",
    "\n",
    "def download_corpus(out_path=DATA_DIR, url=CORPUS_URL, force=False):\n",
    "    pathlib.Path(out_path).mkdir(exist_ok=True)\n",
    "    out_filename = os.path.join(out_path, os.path.basename(url))\n",
    "    \n",
    "    if os.path.isfile(out_filename) and not force:\n",
    "        print(f'Corpus file {out_filename} exists, skipping download.')\n",
    "    else:\n",
    "        print(f'Downloading {url}...')\n",
    "        with urllib.request.urlopen(url) as response, open(out_filename, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response, out_file)\n",
    "        print(f'Saved to {out_filename}.')\n",
    "    return out_filename\n",
    "    \n",
    "corpus_path = download_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odMgUm_S2ESW"
   },
   "source": [
    "Load the text into memory and print a snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-26T22:07:03.276744Z",
     "iopub.status.busy": "2021-12-26T22:07:03.275747Z",
     "iopub.status.idle": "2021-12-26T22:07:03.404403Z",
     "shell.execute_reply": "2021-12-26T22:07:03.404403Z"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1640375394900,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "7UU1VV5w2ESX",
    "outputId": "75b44382-5a20-4bc7-ddad-d4aaee8a218e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 6347703 chars\n",
      "ALLS WELL THAT ENDS WELL\n",
      "\n",
      "by William Shakespeare\n",
      "\n",
      "Dramatis Personae\n",
      "\n",
      "  KING OF FRANCE\n",
      "  THE DUKE OF FLORENCE\n",
      "  BERTRAM, Count of Rousillon\n",
      "  LAFEU, an old lord\n",
      "  PAROLLES, a follower of Bertram\n",
      "  TWO FRENCH LORDS, serving with Bertram\n",
      "\n",
      "  STEWARD, Servant to the Countess of Rousillon\n",
      "  LAVACHE, a clown and Servant to the Countess of Rousillon\n",
      "  A PAGE, Servant to the Countess of Rousillon\n",
      "\n",
      "  COUNTESS OF ROUSILLON, mother to Bertram\n",
      "  HELENA, a gentlewoman protected by the Countess\n",
      "  A WIDOW OF FLORENCE.\n",
      "  DIANA, daughter to the Widow\n",
      "\n",
      "  VIOLENTA, neighbour and friend to the Widow\n",
      "  MARIANA, neighbour and friend to the Widow\n",
      "\n",
      "  Lords, Officers, Soldiers, etc., French and Florentine  \n",
      "\n",
      "SCENE:\n",
      "Rousillon; Paris; Florence; Marseilles\n",
      "\n",
      "ACT I. SCENE 1.\n",
      "Rousillon. The COUNT'S palace\n",
      "\n",
      "Enter BERTRAM, the COUNTESS OF ROUSILLON, HELENA, and LAFEU, all in black\n",
      "\n",
      "  COUNTESS. In delivering my son from me, I bury a second husband.\n",
      "  BERTRAM. And I in going, madam, weep o'er my father's death anew;\n",
      "    but I must attend his Majesty's command, to whom I am now in\n",
      "    ward, evermore in subjection.\n",
      "  LAFEU. You shall find of the King a husband, madam; you, sir, a\n",
      "    father. He that so generally is at all times good must of\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "with open(corpus_path, 'r', encoding='utf-8') as f:\n",
    "    corpus = f.read()\n",
    "\n",
    "print(f'Corpus length: {len(corpus)} chars')\n",
    "print(corpus[7:1234])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "metoWpTc2ESY"
   },
   "source": [
    "### Data Preprocessing\n",
    "<a id=part1_3></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOR-1fZs2ESY"
   },
   "source": [
    "The first thing we'll need is to map from each unique character in the corpus to an index that will represent it in our learning process.\n",
    "\n",
    "**TODO**: Implement the `char_maps()` function in the `hw3/charnn.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-26T22:07:03.408392Z",
     "iopub.status.busy": "2021-12-26T22:07:03.408392Z",
     "iopub.status.idle": "2021-12-26T22:07:03.607867Z",
     "shell.execute_reply": "2021-12-26T22:07:03.607867Z"
    },
    "executionInfo": {
     "elapsed": 1353,
     "status": "ok",
     "timestamp": 1640375396249,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "L4dM1FH42ESZ",
    "outputId": "2026462b-c3b9-4df4-cc8a-a9c9e2d00bf8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '$': 4, '&': 5, \"'\": 6, '(': 7, ')': 8, ',': 9, '-': 10, '.': 11, '0': 12, '1': 13, '2': 14, '3': 15, '4': 16, '5': 17, '6': 18, '7': 19, '8': 20, '9': 21, ':': 22, ';': 23, '<': 24, '?': 25, 'A': 26, 'B': 27, 'C': 28, 'D': 29, 'E': 30, 'F': 31, 'G': 32, 'H': 33, 'I': 34, 'J': 35, 'K': 36, 'L': 37, 'M': 38, 'N': 39, 'O': 40, 'P': 41, 'Q': 42, 'R': 43, 'S': 44, 'T': 45, 'U': 46, 'V': 47, 'W': 48, 'X': 49, 'Y': 50, 'Z': 51, '[': 52, ']': 53, '_': 54, 'a': 55, 'b': 56, 'c': 57, 'd': 58, 'e': 59, 'f': 60, 'g': 61, 'h': 62, 'i': 63, 'j': 64, 'k': 65, 'l': 66, 'm': 67, 'n': 68, 'o': 69, 'p': 70, 'q': 71, 'r': 72, 's': 73, 't': 74, 'u': 75, 'v': 76, 'w': 77, 'x': 78, 'y': 79, 'z': 80, '}': 81, '\\ufeff': 82}\n"
     ]
    }
   ],
   "source": [
    "import hw3.charnn as charnn\n",
    "\n",
    "char_to_idx, idx_to_char = charnn.char_maps(corpus)\n",
    "print(char_to_idx)\n",
    "\n",
    "test.assertEqual(len(char_to_idx), len(idx_to_char))\n",
    "test.assertSequenceEqual(list(char_to_idx.keys()), list(idx_to_char.values()))\n",
    "test.assertSequenceEqual(list(char_to_idx.values()), list(idx_to_char.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4SqcZ922ESZ"
   },
   "source": [
    "Seems we have some strange characters in the corpus that are very rare and are probably due to mistakes.\n",
    "To reduce the length of each tensor we'll need to later represent our chars, it's best to remove them.\n",
    "\n",
    "**TODO**: Implement the `remove_chars()` function in the `hw3/charnn.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-26T22:07:03.610850Z",
     "iopub.status.busy": "2021-12-26T22:07:03.610850Z",
     "iopub.status.idle": "2021-12-26T22:07:04.257153Z",
     "shell.execute_reply": "2021-12-26T22:07:04.256201Z"
    },
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1640375396725,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "3E1jrgAl2ESZ",
    "outputId": "f6a56dfa-8d5a-4ea3-fe9a-e4954d9f0fbe",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 34 chars\n"
     ]
    }
   ],
   "source": [
    "corpus, n_removed = charnn.remove_chars(corpus, ['}','$','_','<','\\ufeff'])\n",
    "print(f'Removed {n_removed} chars')\n",
    "\n",
    "# After removing the chars, re-create the mappings\n",
    "char_to_idx, idx_to_char = charnn.char_maps(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxRHsM6f2ESa"
   },
   "source": [
    "The next thing we need is an **embedding** of the chracters.\n",
    "An embedding is a representation of each token from the sequence as a tensor.\n",
    "For a char-level RNN, our tokens will be chars and we can thus use the simplest possible embedding: encode each char as a **one-hot** tensor. In other words, each char will be represented\n",
    "as a tensor whos length is the total number of unique chars (`V`) which contains all zeros except at the index\n",
    "corresponding to that specific char.\n",
    "\n",
    "**TODO**: Implement the functions `chars_to_onehot()` and `onehot_to_chars()` in the `hw3/charnn.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-26T22:07:04.261143Z",
     "iopub.status.busy": "2021-12-26T22:07:04.261143Z",
     "iopub.status.idle": "2021-12-26T22:07:04.384812Z",
     "shell.execute_reply": "2021-12-26T22:07:04.385810Z"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1640375396726,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "-xJXGVtz2ESa",
    "outputId": "8f293f3f-8222-4130-e670-4b1f4b24bb8c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brine a maiden can season her praise in.\n",
      "   \n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]], dtype=torch.int8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workdir\\DL class\\DL_class\\hw3\\hw3\\charnn.py:85: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:766.)\n",
      "  idx = ((embedded_text == 1).nonzero()[:, 1]).tolist()\n"
     ]
    }
   ],
   "source": [
    "# Wrap the actual embedding functions for calling convenience\n",
    "def embed(text):\n",
    "    return charnn.chars_to_onehot(text, char_to_idx)\n",
    "\n",
    "def unembed(embedding):\n",
    "    return charnn.onehot_to_chars(embedding, idx_to_char)\n",
    "\n",
    "text_snippet = corpus[3104:3148]\n",
    "print(text_snippet)\n",
    "print(embed(text_snippet[0:3]))\n",
    "\n",
    "test.assertEqual(text_snippet, unembed(embed(text_snippet)))\n",
    "test.assertEqual(embed(text_snippet).dtype, torch.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GalbvU-m2ESa"
   },
   "source": [
    "### Dataset Creation\n",
    "<a id=part1_4></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xE6O9TQ2ESb"
   },
   "source": [
    "We wish to train our model to generate text by constantly predicting what the next char should be based on the past.\n",
    "To that end we'll need to train our recurrent network in a way similar to a classification task. At each timestep, we input a char and set the expected output (label) to be the next char in the original sequence.\n",
    "\n",
    "We will split our corpus into shorter sequences of length `S` chars (see question below).\n",
    "Each **sample** we provide our model with will therefore be a tensor of shape `(S,V)` where `V` is the embedding dimension. Our model will operate sequentially on each char in the sequence.\n",
    "For each sample, we'll also need a **label**. This is simply another sequence, shifted by one char so that the label of each char is the next char in the corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HlWrlqsb2ESb"
   },
   "source": [
    "**TODO**: Implement the `chars_to_labelled_samples()` function in the `hw3/charnn.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-26T22:07:04.391824Z",
     "iopub.status.busy": "2021-12-26T22:07:04.390796Z",
     "iopub.status.idle": "2021-12-26T22:07:45.314511Z",
     "shell.execute_reply": "2021-12-26T22:07:45.313500Z"
    },
    "executionInfo": {
     "elapsed": 40008,
     "status": "ok",
     "timestamp": 1640375436728,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "-nA9oo142ESb",
    "outputId": "680432c0-d48f-4c0f-dc23-eeeaa73e0e32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples shape: torch.Size([99182, 64, 78])\n",
      "labels shape: torch.Size([99182, 64])\n"
     ]
    }
   ],
   "source": [
    "# Create dataset of sequences\n",
    "seq_len = 64\n",
    "vocab_len = len(char_to_idx)\n",
    "# Create labelled samples\n",
    "samples, labels = charnn.chars_to_labelled_samples(corpus, char_to_idx, seq_len, device)\n",
    "print(f'samples shape: {samples.shape}')\n",
    "print(f'labels shape: {labels.shape}')\n",
    "\n",
    "# Test shapes\n",
    "num_samples = (len(corpus) - 1) // seq_len\n",
    "test.assertEqual(samples.shape, (num_samples, seq_len, vocab_len))\n",
    "test.assertEqual(labels.shape, (num_samples, seq_len))\n",
    "\n",
    "# Test content\n",
    "for r in range(1000):\n",
    "    # random sample\n",
    "    i = np.random.randint(num_samples, size=(1,))[0]\n",
    "    # Compare to corpus\n",
    "    test.assertEqual(unembed(samples[i]), corpus[i*seq_len:(i+1)*seq_len], msg=f\"content mismatch in sample {i}\")\n",
    "    # Compare to labels\n",
    "    sample_text = unembed(samples[i])\n",
    "    label_text = str.join('', [idx_to_char[j.item()] for j in labels[i]])\n",
    "    test.assertEqual(sample_text[1:], label_text[0:-1], msg=f\"label mismatch in sample {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dg29ioG12ESb"
   },
   "source": [
    "Let's print a few consecutive samples. You should see that the text continues between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-26T22:07:45.317474Z",
     "iopub.status.busy": "2021-12-26T22:07:45.317474Z",
     "iopub.status.idle": "2021-12-26T22:07:45.439149Z",
     "shell.execute_reply": "2021-12-26T22:07:45.438164Z"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1640375436728,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "VaRSefRM2ESc",
    "outputId": "3328f470-73f9-4c39-c387-1fde5465ed4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample [48984]:\n",
      "\tr all the orld, as just as you will desire; and seven hundre\n",
      "sample [48985]:\n",
      "\td pounds of moneys, and gold, and silver, is her grandsire u\n",
      "sample [48986]:\n",
      "\tpon his death's-bed-Got deliver to a joyful resurrections!-g\n",
      "sample [48987]:\n",
      "\tive, when she is able to overtake seventeen years old. It we\n",
      "sample [48988]:\n",
      "\tre a goot motion if we leave our pribbles and prabbles, and\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "i = random.randrange(num_samples-5)\n",
    "for i in range(i, i+5):\n",
    "    test.assertEqual(len(samples[i]), seq_len)\n",
    "    s = re.sub(r'\\s+', ' ', unembed(samples[i])).strip()\n",
    "    print(f'sample [{i}]:\\n\\t{s}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sg1B7Fw-2ESc"
   },
   "source": [
    "As usual, instead of feeding one sample at a time into our model's forward we'll work with **batches** of samples. This means that at every timestep, our model will operate on a batch of chars that are from **different sequences**.\n",
    "Effectively this will allow us to parallelize training our model by dong matrix-matrix multiplications\n",
    "instead of matrix-vector during the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HVFCXDw2ESc"
   },
   "source": [
    "An important nuance is that we need the batches to be **contiguous**, i.e. sample $k$ in batch $j$ should continue sample $k$ from batch $j-1$.\n",
    "The following figure illustrates this:\n",
    "\n",
    "<img src=\"imgs/rnn-batching.png\"/>\n",
    "\n",
    "If we naïvely take consecutive samples into batches, e.g. `[0,1,...,B-1]`, `[B,B+1,...,2B-1]` and so on, we won't have contiguous\n",
    "sequences at the same index between adjacent batches.\n",
    "\n",
    "To accomplish this we need to tell our `DataLoader` which samples to combine together into one batch.\n",
    "We do this by implementing a custom PyTorch `Sampler`, and providing it to our `DataLoader`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOlniQ6X2ESd"
   },
   "source": [
    "**TODO**: Implement the `SequenceBatchSampler` class in the `hw3/charnn.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-26T22:07:45.444137Z",
     "iopub.status.busy": "2021-12-26T22:07:45.443139Z",
     "iopub.status.idle": "2021-12-26T22:07:45.563846Z",
     "shell.execute_reply": "2021-12-26T22:07:45.563846Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1640375436729,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "iwNWHBFX2ESd",
    "outputId": "e1057c0b-2bcb-45ca-cfda-670b8fa79fbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampler_idx =\n",
      " [tensor(0), tensor(1), tensor(2), tensor(3), tensor(4), tensor(5), tensor(6), tensor(7), tensor(8), tensor(9), tensor(10), tensor(11), tensor(12), tensor(13), tensor(14), tensor(15), tensor(16), tensor(17), tensor(18), tensor(19), tensor(20), tensor(21), tensor(22), tensor(23), tensor(24), tensor(25), tensor(26), tensor(27), tensor(28), tensor(29)]\n"
     ]
    }
   ],
   "source": [
    "from hw3.charnn import SequenceBatchSampler\n",
    "\n",
    "sampler = SequenceBatchSampler(dataset=range(32), batch_size=10)\n",
    "sampler_idx = list(sampler)\n",
    "print('sampler_idx =\\n', sampler_idx)\n",
    "\n",
    "# Test the Sampler\n",
    "test.assertEqual(len(sampler_idx), 30)\n",
    "batch_idx = np.array(sampler_idx).reshape(-1, 10)\n",
    "for k in range(10):\n",
    "    test.assertEqual(np.diff(batch_idx[:, k], n=2).item(), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDLPf1G12ESd"
   },
   "source": [
    "Even though we're working with sequences, we can still use the standard PyTorch `Dataset`/`DataLoader` combo.\n",
    "For the dataset we can use a built-in class, `TensorDataset` to return tuples of `(sample, label)`\n",
    "from the `samples` and `labels` tensors we created above.\n",
    "The `DataLoader` will be provided with our custom `Sampler` so that it generates appropriate batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T22:07:45.568828Z",
     "iopub.status.busy": "2021-12-26T22:07:45.567837Z",
     "iopub.status.idle": "2021-12-26T22:07:45.687540Z",
     "shell.execute_reply": "2021-12-26T22:07:45.686513Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1640375436729,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "03m465lt2ESd"
   },
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "\n",
    "# Create DataLoader returning batches of samples.\n",
    "batch_size = 32\n",
    "\n",
    "ds_corpus = torch.utils.data.TensorDataset(samples, labels)\n",
    "sampler_corpus = SequenceBatchSampler(ds_corpus, batch_size)\n",
    "dl_corpus = torch.utils.data.DataLoader(ds_corpus, batch_size=batch_size, sampler=sampler_corpus, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYRYpx8x2ESd"
   },
   "source": [
    "Let's see what that gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-26T22:07:45.690518Z",
     "iopub.status.busy": "2021-12-26T22:07:45.690518Z",
     "iopub.status.idle": "2021-12-26T22:07:45.813176Z",
     "shell.execute_reply": "2021-12-26T22:07:45.813176Z"
    },
    "executionInfo": {
     "elapsed": 497,
     "status": "ok",
     "timestamp": 1640375437221,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "1BMd8Ivz2ESd",
    "outputId": "f0a017c2-e2c3-455e-fe0b-1ad7abc39a98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num batches: 3100\n",
      "shape of a batch of samples: torch.Size([32, 64, 78])\n",
      "shape of a batch of labels: torch.Size([32, 64])\n"
     ]
    }
   ],
   "source": [
    "print(f'num batches: {len(dl_corpus)}')\n",
    "\n",
    "x0, y0 = next(iter(dl_corpus))\n",
    "print(f'shape of a batch of samples: {x0.shape}')\n",
    "print(f'shape of a batch of labels: {y0.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VH04gcs2ESe"
   },
   "source": [
    "Now lets look at the same sample index from multiple batches taken from our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-26T22:07:45.818163Z",
     "iopub.status.busy": "2021-12-26T22:07:45.817165Z",
     "iopub.status.idle": "2021-12-26T22:07:45.956791Z",
     "shell.execute_reply": "2021-12-26T22:07:45.957788Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1640375437222,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "ahyI9ghc2ESe",
    "outputId": "b9ee64ee-b532-474c-8d42-435c470b6d2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== batch 0, sample 10 (torch.Size([64, 78])): ===\n",
      "\tLords, Officers, Soldiers, etc., French and Florentine SCENE:\n",
      "=== batch 1, sample 10 (torch.Size([64, 78])): ===\n",
      "\tat her education promises; her dispositions she inherits, wh\n",
      "=== batch 2, sample 10 (torch.Size([64, 78])): ===\n",
      "\tthat all! I think not on my father; And these great tears gr\n",
      "=== batch 3, sample 10 (torch.Size([64, 78])): ===\n",
      "\town up; marry, in blowing him down again, with the breach\n",
      "=== batch 4, sample 10 (torch.Size([64, 78])): ===\n",
      "\terly better; marry, yet 'tis a wither'd pear. Will you anyth\n"
     ]
    }
   ],
   "source": [
    "# Check that sentences in in same index of different batches complete each other.\n",
    "k = random.randrange(batch_size)\n",
    "for j, (X, y) in enumerate(dl_corpus,):\n",
    "    print(f'=== batch {j}, sample {k} ({X[k].shape}): ===')\n",
    "    s = re.sub(r'\\s+', ' ', unembed(X[k])).strip()\n",
    "    print(f'\\t{s}')\n",
    "    if j==4: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ibxOLsm2ESe"
   },
   "source": [
    "### Model Implementation\n",
    "<a id=part1_5></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zEbjmLXk2ESe"
   },
   "source": [
    "Finally, our data set is ready so we can focus on our model.\n",
    "\n",
    "We'll implement here is a multilayer gated recurrent unit (GRU) model, with dropout.\n",
    "This model is a type of RNN which performs similar to the well-known LSTM model,\n",
    "but it's somewhat easier to train because it has less parameters.\n",
    "We'll modify the regular GRU slightly by applying dropout to\n",
    "the hidden states passed between layers of the model.\n",
    "\n",
    "The model accepts an input $\\mat{X}\\in\\set{R}^{S\\times V}$ containing a sequence of embedded chars.\n",
    "It returns an output $\\mat{Y}\\in\\set{R}^{S\\times V}$ of predictions for the next char and the final hidden state\n",
    "$\\mat{H}\\in\\set{R}^{L\\times H}$. Here $S$ is the sequence length, $V$ is the vocabulary size (number of unique chars), $L$ is the number of layers in the model and $H$ is the hidden dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBVTEWvh2ESe"
   },
   "source": [
    "Mathematically, the model's forward function at layer $k\\in[1,L]$ and timestep $t\\in[1,S]$ can be described as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\vec{z_t}^{[k]} &= \\sigma\\left(\\vec{x}^{[k]}_t {\\mattr{W}_{\\mathrm{xz}}}^{[k]} +\n",
    "    \\vec{h}_{t-1}^{[k]} {\\mattr{W}_{\\mathrm{hz}}}^{[k]} + \\vec{b}_{\\mathrm{z}}^{[k]}\\right) \\\\\n",
    "\\vec{r_t}^{[k]} &= \\sigma\\left(\\vec{x}^{[k]}_t {\\mattr{W}_{\\mathrm{xr}}}^{[k]} +\n",
    "    \\vec{h}_{t-1}^{[k]} {\\mattr{W}_{\\mathrm{hr}}}^{[k]} + \\vec{b}_{\\mathrm{r}}^{[k]}\\right) \\\\\n",
    "\\vec{g_t}^{[k]} &= \\tanh\\left(\\vec{x}^{[k]}_t {\\mattr{W}_{\\mathrm{xg}}}^{[k]} +\n",
    "    (\\vec{r_t}^{[k]}\\odot\\vec{h}_{t-1}^{[k]}) {\\mattr{W}_{\\mathrm{hg}}}^{[k]} + \\vec{b}_{\\mathrm{g}}^{[k]}\\right) \\\\\n",
    "\\vec{h_t}^{[k]} &= \\vec{z}^{[k]}_t \\odot \\vec{h}^{[k]}_{t-1} + \\left(1-\\vec{z}^{[k]}_t\\right)\\odot \\vec{g_t}^{[k]}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jgy5ogvx2ESf"
   },
   "source": [
    "The input to each layer is,\n",
    "$$\n",
    "\\mat{X}^{[k]} =\n",
    "\\begin{bmatrix}\n",
    "    {\\vec{x}_1}^{[k]} \\\\ \\vdots \\\\ {\\vec{x}_S}^{[k]}\n",
    "\\end{bmatrix} \n",
    "=\n",
    "\\begin{cases}\n",
    "    \\mat{X} & \\mathrm{if} ~k = 1~ \\\\\n",
    "    \\mathrm{dropout}_p \\left(\n",
    "    \\begin{bmatrix}\n",
    "        {\\vec{h}_1}^{[k-1]} \\\\ \\vdots \\\\ {\\vec{h}_S}^{[k-1]}\n",
    "    \\end{bmatrix} \\right) & \\mathrm{if} ~1 < k \\leq L+1~\n",
    "\\end{cases}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqMglGwi2ESf"
   },
   "source": [
    "The output of the entire model is then,\n",
    "$$\n",
    "\\mat{Y} = \\mat{X}^{[L+1]} {\\mattr{W}_{\\mathrm{hy}}} + \\mat{B}_{\\mathrm{y}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8IqUiaD42ESf"
   },
   "source": [
    "and the final hidden state is\n",
    "$$\n",
    "\\mat{H} = \n",
    "\\begin{bmatrix}\n",
    "    {\\vec{h}_S}^{[1]} \\\\ \\vdots \\\\ {\\vec{h}_S}^{[L]}\n",
    "\\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWqZ2VzO2ESf"
   },
   "source": [
    "Notes:\n",
    "- $t\\in[1,S]$ is the timestep, i.e. the current position within the sequence of each sample.\n",
    "- $\\vec{x}_t^{[k]}$ is the input of layer $k$ at timestep $t$, respectively.\n",
    "- The outputs of the **last layer** $\\vec{y}_t^{[L]}$, are the predicted next characters for every input char.\n",
    "  These are similar to class scores in classification tasks.\n",
    "- The hidden states at the **last timestep**, $\\vec{h}_S^{[k]}$, are the final hidden state returned from the model.\n",
    "- $\\sigma(\\cdot)$ is the sigmoid function, i.e. $\\sigma(\\vec{z}) = 1/(1+e^{-\\vec{z}})$ which returns values in $(0,1)$.\n",
    "- $\\tanh(\\cdot)$ is the hyperbolic tangent, i.e. $\\tanh(\\vec{z}) = (e^{2\\vec{z}}-1)/(e^{2\\vec{z}}+1)$ which returns values in $(-1,1)$.\n",
    "- $\\vec{h_t}^{[k]}$ is the hidden state of layer $k$ at time $t$. This can be thought of as the memory of that layer.\n",
    "- $\\vec{g_t}^{[k]}$ is the candidate hidden state for time $t+1$.\n",
    "- $\\vec{z_t}^{[k]}$ is known as the update gate. It combines the previous state with the input to determine how much the current state will be combined with the new candidate state. For example, if $\\vec{z_t}^{[k]}=\\vec{1}$ then the current input has no effect on the output.\n",
    "- $\\vec{r_t}^{[k]}$ is known as the reset gate. It combines the previous state with the input to determine how much of the previous state will affect the current state candidate. For example if $\\vec{r_t}^{[k]}=\\vec{0}$ the previous state has no effect on the current candidate state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGrsGkWS2ESf"
   },
   "source": [
    "Here's a graphical representation of the GRU's forward pass at each timestep. The $\\vec{\\tilde{h}}$ in the image is our $\\vec{g}$ (candidate next state).\n",
    "\n",
    "<img src=\"imgs/gru_cell.png\" width=\"400\"/>\n",
    "\n",
    "You can see how the reset and update gates allow the model to completely ignore it's previous state, completely ignore it's input, or any mixture of those states (since the gates are actually continuous and between $(0,1)$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnSuFMOc2ESf"
   },
   "source": [
    "Here's a graphical representation of the entire model.\n",
    "You can ignore the $c_t^{[k]}$ (cell state) variables (which are relevant for LSTM models).\n",
    "Our model has only the hidden state, $h_t^{[k]}$. Also notice that we added dropout between layers (i.e., on the up arrows).\n",
    "\n",
    "<img src=\"imgs/lstm_model.png\" />\n",
    "\n",
    "The purple tensors are inputs (a sequence and initial hidden state per layer), and the green tensors are outputs (another sequence and final hidden state per layer). Each blue block implements the above forward equations.\n",
    "Blocks that are on the same vertical level are at the same layer, and therefore share parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vt5ZRNAe2ESg"
   },
   "source": [
    "**TODO**: Implement the `MultilayerGRU` class in the `hw3/charnn.py` module.\n",
    "\n",
    "Notes:\n",
    "- You'll need to handle input **batches** now.\n",
    "  The math is identical to the above, but all the tensors will have an extra batch\n",
    "  dimension as their first dimension.\n",
    "- Use the diagram above to help guide your implementation.\n",
    "  It will help you visualize what shapes to returns where, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-26T22:07:45.964769Z",
     "iopub.status.busy": "2021-12-26T22:07:45.963771Z",
     "iopub.status.idle": "2021-12-26T22:07:46.272945Z",
     "shell.execute_reply": "2021-12-26T22:07:46.273943Z"
    },
    "executionInfo": {
     "elapsed": 399,
     "status": "ok",
     "timestamp": 1640375437617,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "iIlWe9YG2ESg",
    "outputId": "54a1c88e-d932-4b54-9662-4cff6409ea15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerGRU(\n",
      "  (xz_0): Linear(in_features=78, out_features=256, bias=False)\n",
      "  (hz_0): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (xr_0): Linear(in_features=78, out_features=256, bias=False)\n",
      "  (hr_0): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (xg_0): Linear(in_features=78, out_features=256, bias=False)\n",
      "  (hg_0): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (xz_1): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (hz_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (xr_1): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (hr_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (xg_1): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (hg_1): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (xz_2): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (hz_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (xr_2): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (hr_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (xg_2): Linear(in_features=256, out_features=256, bias=False)\n",
      "  (hg_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  (out): Linear(in_features=256, out_features=78, bias=True)\n",
      ")\n",
      "x0 shape torch.Size([32, 64, 78])\n",
      "torch.Size([32, 64, 78])\n",
      "y.shape=torch.Size([32, 64, 78])\n",
      "h.shape=torch.Size([32, 3, 256])\n"
     ]
    }
   ],
   "source": [
    "in_dim = vocab_len\n",
    "h_dim = 256\n",
    "n_layers = 3\n",
    "model = charnn.MultilayerGRU(in_dim, h_dim, out_dim=in_dim, n_layers=n_layers)\n",
    "model = model.to(device)\n",
    "print(model)\n",
    "\n",
    "# Test forward pass\n",
    "print(f'x0 shape {x0.shape}')\n",
    "y, h = model(x0.to(dtype=torch.float, device=device))\n",
    "print(y.shape)\n",
    "print(f'y.shape={y.shape}')\n",
    "print(f'h.shape={h.shape}')\n",
    "\n",
    "test.assertEqual(y.shape, (batch_size, seq_len, vocab_len))\n",
    "test.assertEqual(h.shape, (batch_size, n_layers, h_dim))\n",
    "test.assertEqual(len(list(model.parameters())), 9 * n_layers + 2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JpftsW5J2ESg"
   },
   "source": [
    "### Generating text by sampling\n",
    "<a id=part1_6></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wt_FzgNl2ESg"
   },
   "source": [
    "Now that we have a model, we can implement **text generation** based on it.\n",
    "The idea is simple:\n",
    "At each timestep our model receives one char $x_t$ from the input sequence and outputs scores $y_t$\n",
    "for what the next char should be.\n",
    "We'll convert these scores into a probability over each of the possible chars.\n",
    "In other words, for each input char $x_t$ we create a probability distribution for the next char\n",
    "conditioned on the current one and the state of the model (representing all previous inputs):\n",
    "$$p(x_{t+1}|x_t, \\vec{h}_t).$$\n",
    "\n",
    "Once we have such a distribution, we'll sample a char from it.\n",
    "This will be the first char of our generated sequence.\n",
    "Now we can feed this new char into the model, create another distribution, sample the next char and so on.\n",
    "Note that it's crucial to propagate the hidden state when sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkrvQyy22ESg"
   },
   "source": [
    "The important point however is how to create the distribution from the scores.\n",
    "One way, as we saw in previous ML tasks, is to use the softmax function.\n",
    "However, a drawback of softmax is that it can generate very diffuse (more uniform) distributions if the score values are very similar. When sampling, we would prefer to control the distributions and make them less uniform to increase the chance of sampling the char(s) with the highest scores compared to the others.\n",
    "\n",
    "To control the variance of the distribution, a common trick is to add a hyperparameter $T$, known as the \n",
    "*temperature* to the softmax function. The class scores are simply scaled by $T$ before softmax is applied:\n",
    "$$\n",
    "\\mathrm{softmax}_T(\\vec{y}) = \\frac{e^{\\vec{y}/T}}{\\sum_k e^{y_k/T}}\n",
    "$$\n",
    "\n",
    "A low $T$ will result in less uniform distributions and vice-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QYphobrb2ESg"
   },
   "source": [
    "**TODO**: Implement the `hot_softmax()` function in the `hw3/charnn.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "execution": {
     "iopub.execute_input": "2021-12-26T22:07:46.279928Z",
     "iopub.status.busy": "2021-12-26T22:07:46.278931Z",
     "iopub.status.idle": "2021-12-26T22:07:46.434513Z",
     "shell.execute_reply": "2021-12-26T22:07:46.435511Z"
    },
    "executionInfo": {
     "elapsed": 844,
     "status": "ok",
     "timestamp": 1640375438459,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "lVfef9ED2ESh",
    "outputId": "badc3024-3f2f-483b-9265-8bcfdb674124"
   },
   "outputs": [],
   "source": [
    "scores = y[0,0,:].detach()\n",
    "_, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "for t in reversed([0.3, 0.5, 1.0, 100]):\n",
    "    ax.plot(charnn.hot_softmax(scores, temperature=t).cpu().numpy(), label=f'T={t}')\n",
    "ax.set_xlabel('$x_{t+1}$')\n",
    "ax.set_ylabel('$p(x_{t+1}|x_t)$')\n",
    "ax.legend()\n",
    "\n",
    "uniform_proba = 1/len(char_to_idx)\n",
    "uniform_diff = torch.abs(charnn.hot_softmax(scores, temperature=100) - uniform_proba)\n",
    "test.assertTrue(torch.all(uniform_diff < 1e-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTFyZmqP2ESh"
   },
   "source": [
    "**TODO**: Implement the `generate_from_model()` function in the `hw3/charnn.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-26T22:07:46.439499Z",
     "iopub.status.busy": "2021-12-26T22:07:46.439499Z",
     "iopub.status.idle": "2021-12-26T22:07:46.702795Z",
     "shell.execute_reply": "2021-12-26T22:07:46.702795Z"
    },
    "executionInfo": {
     "elapsed": 5282,
     "status": "ok",
     "timestamp": 1640375443736,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "Z8kt7LyW2ESh",
    "outputId": "17a43904-735e-4e21-e664-a8af19d075f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foobar4qN-Jf.bOwD(bW1(QO'.hcB\n",
      "f]FdoHwh&r.jPef2]ykX\n",
      "foobar9\"Wbb0kR.5P)r:cwXj;P!N;.4Kt\"GAF?o\"k6QJ[YAIRm\n",
      "foobar T(-u;ON9Aty4bypRnZcHeQMw8MvB!ggEEU5kVBDeBmY\n"
     ]
    }
   ],
   "source": [
    "for _ in range(3):\n",
    "    text = charnn.generate_from_model(model, \"foobar\", 50, (char_to_idx, idx_to_char), T=0.5)\n",
    "    print(text)\n",
    "\n",
    "    test.assertEqual(len(text), 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghjts99C2ESh"
   },
   "source": [
    "### Training\n",
    "<a id=part1_7></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmz0ofaS2ESh"
   },
   "source": [
    "To train this model, we'll calculate the loss at each time step by comparing the predicted char to\n",
    "the actual char from our label. We can use cross entropy since per char it's similar to a classification problem.\n",
    "We'll then sum the losses over the sequence and back-propagate the gradients though time.\n",
    "Notice that the back-propagation algorithm will \"visit\" each layer's parameter tensors multiple times,\n",
    "so we'll accumulate gradients in parameters of the blocks. Luckily `autograd` will handle this part for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mb5HHYo92ESh"
   },
   "source": [
    "As usual, the first step of training will be to try and **overfit** a large model (many parameters) to a tiny dataset.\n",
    "Again, this is to ensure the model and training code are implemented correctly, i.e. that the model can learn.\n",
    "\n",
    "For a generative model such as this, overfitting is slightly trickier than for classification.\n",
    "What we'll aim to do is to get our model to **memorize** a specific sequence of chars, so that when given the first\n",
    "char in the sequence it will immediately spit out the rest of the sequence verbatim.\n",
    "\n",
    "Let's create a tiny dataset to memorize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-26T22:07:46.707782Z",
     "iopub.status.busy": "2021-12-26T22:07:46.706784Z",
     "iopub.status.idle": "2021-12-26T22:07:46.846411Z",
     "shell.execute_reply": "2021-12-26T22:07:46.847408Z"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1640375443737,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "XzgjM18i2ESh",
    "outputId": "e8df18e4-821e-4808-a561-303cc9c9ea21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text to \"memorize\":\n",
      "\n",
      "TRAM. What would you have?\n",
      "  HELENA. Something; and scarce so much; nothing, indeed.\n",
      "    I would not tell you what I would, my lord.\n",
      "    Faith, yes:\n",
      "    Strangers and foes do sunder and not kiss.\n",
      "  BERTRAM. I pray you, stay not, but in haste to horse.\n",
      "  HE\n"
     ]
    }
   ],
   "source": [
    "# Pick a tiny subset of the dataset\n",
    "subset_start, subset_end = 1001, 1005\n",
    "ds_corpus_ss = torch.utils.data.Subset(ds_corpus, range(subset_start, subset_end))\n",
    "batch_size_ss = 1\n",
    "sampler_ss = SequenceBatchSampler(ds_corpus_ss, batch_size=batch_size_ss)\n",
    "dl_corpus_ss = torch.utils.data.DataLoader(ds_corpus_ss, batch_size_ss, sampler=sampler_ss, shuffle=False)\n",
    "\n",
    "# Convert subset to text\n",
    "subset_text = ''\n",
    "for i in range(subset_end - subset_start):\n",
    "    subset_text += unembed(ds_corpus_ss[i][0])\n",
    "print(f'Text to \"memorize\":\\n\\n{subset_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ii6yQRhA2ESi"
   },
   "source": [
    "Now let's implement the first part of our training code.\n",
    "\n",
    "**TODO**: Implement the `train_epoch()` and `train_batch()` methods of the `RNNTrainer` class in the `hw3/training.py` module. \n",
    "You must think about how to correctly handle the hidden state of the model between batches and epochs for this specific task (i.e. text generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-26T22:07:46.857382Z",
     "iopub.status.busy": "2021-12-26T22:07:46.857382Z",
     "iopub.status.idle": "2021-12-26T22:08:07.528327Z",
     "shell.execute_reply": "2021-12-26T22:08:07.529341Z"
    },
    "executionInfo": {
     "elapsed": 136685,
     "status": "ok",
     "timestamp": 1640375580409,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "w9doxEJf2ESi",
    "outputId": "9584ce6f-3b8f-4455-ac80-b710718bba57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAM. What would you have?\n",
      "  HELENA. Something; and scarce so much; nothing, indeed.\n",
      "    I would not tell you what I would, my lord.\n",
      "    Faith, yes:\n",
      "    Strangers and foes do sunder and not kiss.\n",
      "  BERTRAM. I pray you, stay not, but in haste to horse.\n",
      "  HE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Workdir\\DL class\\DL_class\\hw3\\hw3\\training.py:273: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  num_correct = torch.sum(torch.tensor(pred.argmax(dim=1).eq(y)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch #1: Avg. loss = 3.940, Accuracy = 17.58%\n",
      "Tt                                                                                                                                                                                                                                           o                  \n",
      "\n",
      "Epoch #25: Avg. loss = 0.268, Accuracy = 96.09%\n",
      "TAM. What would you have?\n",
      "  HELENA. Something; and scarce so much; nothing, indeed.\n",
      "    I would not would you wand not kiss.\n",
      "  BERTRAM. I pray you, stay not, but in haste to horse.\n",
      "  HELERAM. I pray you, stay not, but in haste to horse.\n",
      "  HELERAM. I pray y\n",
      "\n",
      "Epoch #50: Avg. loss = 0.008, Accuracy = 100.00%\n",
      "TRAM. What would you have?\n",
      "  HELENA. Something; and scarce so much; nothing, indeed.\n",
      "    I would not tell you what I would, my lord.\n",
      "    Faith, yes:\n",
      "    Strangers and foes do sunder and not kiss.\n",
      "  BERTRAM. I pray you, stay not, but in haste to horse.\n",
      "  HE\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from hw3.training import RNNTrainer\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "lr = 0.01\n",
    "num_epochs = 500\n",
    "\n",
    "in_dim = vocab_len\n",
    "h_dim = 128\n",
    "n_layers = 2\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "model = charnn.MultilayerGRU(in_dim, h_dim, out_dim=in_dim, n_layers=n_layers).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "trainer = RNNTrainer(model, loss_fn, optimizer, device)\n",
    "print(subset_text)\n",
    "for epoch in range(num_epochs):\n",
    "    # print(dl_corpus_ss[0])\n",
    "    epoch_result = trainer.train_epoch(dl_corpus_ss, verbose=False)\n",
    "    # print('done')\n",
    "    # Every X epochs, we'll generate a sequence starting from the first char in the first sequence\n",
    "    # to visualize how/if/what the model is learning.\n",
    "    if epoch == 0 or (epoch+1) % 25 == 0:\n",
    "        avg_loss = np.mean(epoch_result.losses)\n",
    "        accuracy = np.mean(epoch_result.accuracy)\n",
    "        print(f'\\nEpoch #{epoch+1}: Avg. loss = {avg_loss:.3f}, Accuracy = {accuracy:.2f}%')\n",
    "        \n",
    "        generated_sequence = charnn.generate_from_model(model, subset_text[0],\n",
    "                                                        seq_len*(subset_end-subset_start),\n",
    "                                                        (char_to_idx,idx_to_char), T=0.1)\n",
    "        \n",
    "        # Stop if we've successfully memorized the small dataset.\n",
    "        print(generated_sequence)\n",
    "        if generated_sequence == subset_text:\n",
    "            break\n",
    "\n",
    "# Test successful overfitting\n",
    "test.assertGreater(epoch_result.accuracy, 99)\n",
    "test.assertEqual(generated_sequence, subset_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "piw96c6U2ESi"
   },
   "source": [
    "OK, so training works - we can memorize a short sequence.\n",
    "We'll now train a much larger model on our large dataset. You'll need a GPU for this part.\n",
    "\n",
    "First, lets set up our dataset and models for training.\n",
    "We'll split our corpus into 90% train and 10% test-set.\n",
    "Also, we'll use a learning-rate scheduler to control the learning rate during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1QfJMHk2ESi"
   },
   "source": [
    "**TODO**: Set the hyperparameters in the `part1_rnn_hyperparams()` function of the `hw3/answers.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-26T22:08:07.539314Z",
     "iopub.status.busy": "2021-12-26T22:08:07.538295Z",
     "iopub.status.idle": "2021-12-26T22:08:48.977952Z",
     "shell.execute_reply": "2021-12-26T22:08:48.978950Z"
    },
    "executionInfo": {
     "elapsed": 27497,
     "status": "ok",
     "timestamp": 1640375607903,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "P1CtiWiG2ESi",
    "outputId": "dd7cc445-fcac-4252-eb9b-836434ae3cfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparams:\n",
      " {'batch_size': 128, 'seq_len': 50, 'h_dim': 128, 'n_layers': 3, 'dropout': 0.3, 'learn_rate': 0.0001, 'lr_sched_factor': 0.2, 'lr_sched_patience': 5}\n",
      "Train: 892 batches, 5708800 chars\n",
      "Test:   99 batches,  633600 chars\n"
     ]
    }
   ],
   "source": [
    "from hw3.answers import part1_rnn_hyperparams\n",
    "\n",
    "hp = part1_rnn_hyperparams()\n",
    "print('hyperparams:\\n', hp)\n",
    "### Dataset definition\n",
    "vocab_len = len(char_to_idx)\n",
    "batch_size = hp['batch_size']\n",
    "seq_len = hp['seq_len']\n",
    "train_test_ratio = 0.9\n",
    "num_samples = (len(corpus) - 1) // seq_len\n",
    "num_train = int(train_test_ratio * num_samples)\n",
    "\n",
    "samples, labels = charnn.chars_to_labelled_samples(corpus, char_to_idx, seq_len, device)\n",
    "\n",
    "ds_train = torch.utils.data.TensorDataset(samples[:num_train], labels[:num_train])\n",
    "sampler_train = SequenceBatchSampler(ds_train, batch_size)\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size, shuffle=False, sampler=sampler_train, drop_last=True)\n",
    "\n",
    "ds_test = torch.utils.data.TensorDataset(samples[num_train:], labels[num_train:])\n",
    "sampler_test = SequenceBatchSampler(ds_test, batch_size)\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size, shuffle=False, sampler=sampler_test, drop_last=True)\n",
    "\n",
    "print(f'Train: {len(dl_train):3d} batches, {len(dl_train)*batch_size*seq_len:7d} chars')\n",
    "print(f'Test:  {len(dl_test):3d} batches, {len(dl_test)*batch_size*seq_len:7d} chars')\n",
    "\n",
    "### Training definition\n",
    "in_dim = out_dim = vocab_len\n",
    "checkpoint_file = 'checkpoints/final.pt'\n",
    "num_epochs = 50\n",
    "early_stopping = 5\n",
    "\n",
    "model = charnn.MultilayerGRU(in_dim, hp['h_dim'], out_dim, hp['n_layers'], hp['dropout'])\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=hp['learn_rate'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', factor=hp['lr_sched_factor'], patience=hp['lr_sched_patience'], verbose=True\n",
    ")\n",
    "trainer = RNNTrainer(model, loss_fn, optimizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h89GEiKj2ESj"
   },
   "source": [
    "The code blocks below will train the model and save checkpoints containing the training state and the best model parameters to a file. This allows you to stop training and resume it later from where you left.\n",
    "\n",
    "Note that you can use the `main.py` script provided within the assignment folder to run this notebook from the command line as if it were a python script by using the `run-nb` subcommand. This allows you to train your model using this notebook without starting jupyter. You can combine this with `srun` or `sbatch` to run the notebook with a GPU on the course servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USMBGlEA2ESj"
   },
   "source": [
    "**TODO**:\n",
    "- Implement the `fit()` method of the `Trainer` class. You can reuse the relevant implementation parts from HW2, but make sure to implement early stopping and checkpoints.\n",
    "- Implement the `test_epoch()` and `test_batch()` methods of the `RNNTrainer` class in the `hw3/training.py` module.\n",
    "- Run the following block to train.\n",
    "- When training is done and you're satisfied with the model's outputs, rename the checkpoint file to `checkpoints/rnn_final.pt`.\n",
    "  This will cause the block to skip training and instead load your saved model when running the homework submission script.\n",
    "  Note that your submission zip file will not include the checkpoint file. This is OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2021-12-26T22:08:48.983936Z",
     "iopub.status.busy": "2021-12-26T22:08:48.982939Z",
     "iopub.status.idle": "2021-12-26T22:08:49.116582Z",
     "shell.execute_reply": "2021-12-26T22:08:49.116582Z"
    },
    "executionInfo": {
     "elapsed": 841,
     "status": "ok",
     "timestamp": 1640375608733,
     "user": {
      "displayName": "Bar Cohen",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "08466736935148352038"
     },
     "user_tz": -120
    },
    "id": "Om6mCxKN2ESj",
    "outputId": "3b951785-d4ec-40cf-a9cb-ff3e8ca8db56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Loading final checkpoint file checkpoints/final.pt instead of training\n"
     ]
    }
   ],
   "source": [
    "from cs3600.plot import plot_fit\n",
    "\n",
    "def post_epoch_fn(epoch, train_res, test_res, verbose):\n",
    "    # Update learning rate\n",
    "    scheduler.step(test_res.accuracy)\n",
    "    # Sample from model to show progress\n",
    "    if verbose:\n",
    "        start_seq = \"ACT I.\"\n",
    "        generated_sequence = charnn.generate_from_model(\n",
    "            model, start_seq, 100, (char_to_idx,idx_to_char), T=0.5\n",
    "        )\n",
    "        print(generated_sequence)\n",
    "\n",
    "# Train, unless final checkpoint is found\n",
    "checkpoint_file_final = checkpoint_file\n",
    "\n",
    "\n",
    "if os.path.isfile(checkpoint_file_final):\n",
    "    print(f'*** Loading final checkpoint file {checkpoint_file_final} instead of training')\n",
    "    saved_state = torch.load(checkpoint_file_final, map_location=device)\n",
    "    model.load_state_dict(saved_state['model_state'])\n",
    "else:\n",
    "  try:\n",
    "      # Print pre-training sampling\n",
    "      print(charnn.generate_from_model(model, \"ACT I.\", 100, (char_to_idx,idx_to_char), T=0.5))\n",
    "\n",
    "      fit_res = trainer.fit(dl_train, dl_test, 50, max_batches=None,\n",
    "                            post_epoch_fn=post_epoch_fn, early_stopping=early_stopping,\n",
    "                            checkpoints='A_new_model2', print_every=1)\n",
    "      \n",
    "      fig, axes = plot_fit(fit_res)\n",
    "  except KeyboardInterrupt as e:\n",
    "      print('\\n *** Training interrupted by user')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlUXj-xn2ESj"
   },
   "source": [
    "### Generating a work of art\n",
    "<a id=part1_8></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xMFgvWfC2ESj"
   },
   "source": [
    "Armed with our fully trained model, let's generate the next Hamlet! You should experiment with modifying the sampling temperature and see what happens.\n",
    "\n",
    "The text you generate should “look” like a Shakespeare play:\n",
    "old-style English words and sentence structure, directions for the actors\n",
    "(like “Exit/Enter”), sections (Act I/Scene III) etc.\n",
    "There will be no coherent plot of course, but it should at least seem like\n",
    "a Shakespearean play when not looking too closely.\n",
    "If this is not what you see, go back, debug and/or and re-train.\n",
    "\n",
    "**TODO**: Specify the generation parameters in the `part1_generation_params()` function within the `hw3/answers.py` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T22:08:49.122566Z",
     "iopub.status.busy": "2021-12-26T22:08:49.121569Z",
     "iopub.status.idle": "2021-12-26T22:08:59.965205Z",
     "shell.execute_reply": "2021-12-26T22:08:59.965205Z"
    },
    "id": "a8_Ddst62ESk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the world's a stage the eire e  the e the  eother the sard \n",
      "   The stail  the word  the stele  to the world to   to the  ea the r ord to the seaver \n",
      "    The sord the eere tn tour soall tou shall the saaee  the e to  the sase the e the e the sane tore the sare to to tore the e the eiou the stale the e to mor toee the seart the sore the e tou shard \n",
      "  AARTING. What so  the say the e the sore  the r sone the e to sone the eear  the sore the r sers  to the son \n",
      "or the seep\n",
      "\n",
      "GUCHNTER:\n",
      "And the soall tou  shall sou   the say the saare \n",
      "\n",
      "o the sore the sang the shall the sore  to tour hare the sarded toe   to the saaee the spien  the seall the soee the soare \n",
      "    The soeet the sanger tor the seare tn the ware to the e ere the sone  the seat  the sane \n",
      "   UCENTER. What well to me to the  and the serve the  toue the senver \n",
      "  o me e the sart  to  the e the  the sone the e the son \n",
      "o the sare the start the  so should the sart \n",
      "  o e the sand the e th te the sarte the say to the sird oo the soul to toee to the soall the woll  the soall so the seall \n",
      "\n",
      "GROCCER:\n",
      "What shall not  tou  shall the  the souer    tor tour \n",
      "   AINING I will toe thee  to tore to the searer the soul of the cand \n",
      "    The tere the comperit to ma lord of the sord the wordd the  ear the suall the seale \n",
      "n the sert the e the soeen the seeve the ward to  the sare \n",
      "    Thet soe the soarl  n the seeve the see the stand  the wordher  the sare  the wordd     toe e the wane the secee  the son  the word to the sueak tou  bear   the saart \n",
      "  ANTONIUS. The shall be the  to toe   tor  the barth  the shale the seed to the shall the e toe  the sing the sarther \n",
      "    Ao to  the e thee to  the sare to  the e o  tor toue \n",
      "    The seall the stand the seep to tou  and the sare to the seet ng the e tour so soe the sand \n",
      "\n",
      "  ARDIAING. The  shall so the coust to the son the sing the  to to me e the sord of the saae the   the e the e the saster the seall the some the worth the servent the shall boe the  the sarti\n",
      "   AnTHARDES. The sart  the  the wort the sore the seep tou  tare to mour the soul thee \n",
      "\n",
      "   The  the shall the surp to me the sould \n",
      "   UTHERD IIGO. Wo the sea the e thet the some tou   and the shall the  tore th toe to she see the see \n",
      "    The say  o toe the say the e  the saall to  the sare to  the sarte to ter  to the seall the see to the shall so some to toe sear \n",
      "    The sirth th e the see tnd the sore the crople tou the sorer to the stann the cane the sarth the saall the sould\n",
      "  the  ord \n",
      "  the surll the sart the e the  the eea  th the strenge\n",
      " the saae the sare  to   to the sone \n",
      "\n",
      "SIENG HICHARD:\n",
      "The e to ma lord the sear  the soul the e the soar  the seeet th tee  tor  the sert to tor the eore tou d to the sone the start the sart to mane the saere to the coust to the e the sord the e the wart the   the shall tou hou  the say tou are the  the seall the e the     the soue the e the stard the  the shall to toe tour so man     ANDORUS. The eare the word  the e ore toe the part the such to the e the   oue the soaee the canger the r the soul to the sander toe souer to the eare t the e \n",
      "    The e tou shall the wear to the e the e the siae  tnd the sare to  the  ear  the sire the soee  the e the sand \n",
      "   Ahe  tear  o toe the sone one oe the seet the sight tourd \n",
      "o tou with the counter \n",
      "ne  to the e the  the sould \n",
      "\n",
      "   The eine to tour tore the e to tour  to the  the saart the should the son  ou the sord the sone tou the e the carthe\n",
      "   ARSIAS. What so moe tour tour the sand the seat   tor  the sord the  thethe  th the brother thet toe  to  the see   the rart the steat  the saree the searth \n",
      "\n",
      "GINGARUS:\n",
      "And the want  to me the seeet \n",
      "    The  ane the sucher  the see th to e th e the  tour see the soue the   ould the man to my lord,\n",
      "and tou so my love \n",
      "o  or  thet we  the r stand the soall the  the wander the world the words to the send th tee comnte the e of the soeak to the sterd \n",
      "\n",
      "GUOUC IICGER:\n",
      "\n",
      "\n",
      "SCENE EN\n",
      "\n",
      "GORD:\n",
      "IN\n",
      "IA:\n",
      "\n",
      "The some to the sing tor the say the soate the sould the e the r seng  to  the e anl the soue \n",
      "    The soeak  ou shall be the rane the sord the sear to toe tore the sord th the eeare the saree the santer the staand the sear the e the soul  the e on the seeak the sere the sarter\n",
      "\n",
      "ANTINIUS:\n",
      "The e the way the   oe the seep th the  the be tot the  tole the courted the  then  our the e ore the sang the sang  the such the seep the world to the conterd thet the e the say the son to the souee the e toe the stayd \n",
      "    The e the the   tould the sine  o the woll to   the r toe the sand th the srown the sord the seep the sare the rart \n",
      "   ARIING. Why  the sord the   to the some the sath the sart the surper   the stann the say the shall soae to the eart the say \n",
      "    The songer the saall the e the e thethe the shake the sange \n",
      "    The seald to  tou shall the sare the say the contle s th the r ford of the sord the stall tou  tour the e o to the  art the sare  thet the wear  the counted \n",
      "    The sould the e the sane the song  ou toe son  the  the sane  to   the eear the sart the servent  the senter the eaand tour hone \n",
      "    To the son the sane  the say to the r stand tnd the word the seep the word to the son the e th the coult to mo th the sear to the e eer the seall and sart \n",
      "    Tnd the e the  eat the seeat to the rore to the sord to  tour that to the word.\n",
      "\n",
      "GOOCC OFFERD IIR\n",
      " \n",
      "\n",
      "KUCHENTE\n",
      "\n",
      "\n",
      "ay the word to  the soul  the  and toe e tou will the shaend the sand toue  oue the soul \n",
      "    The sart  to the shall the staie the sore the soue the soall the saee the soall the say the soall the r the soner the world the sore \n",
      "    The sath r to the   the sare th the  oould to the seep the e th the eear to  the seed tnd the sire the r ting the sare the seep the seall  thet thete tou soe   and toee tor the e th the  the son to the seep to the e the stand tou the saee the wand \n",
      "    The seall tou so the seall so e the e the word   thet  ou  the sword the soere to e tord \n",
      "  PAONTERS. The sear   to tou the man  o   the shall the word the e tee  the  tould  the sould the sang the sanger the shall the sane the sorle the countle the soue   the  the son the e to the sanger\n",
      " o  Tha shall be the wordd the e the seall  the soare the woart the r sone the sangee the see the seare the soall \n",
      "  the soell tour soall \n",
      "   the shall the rone to the word to tour \n",
      "and the  ere the worl the say \n",
      "\n",
      "   Th the stard the soare to  the seave the son  the see tnd toue  or the eord tor  the saye the e the soue to  the  tour haae \n",
      " the e the sord the  toue the r toe toue the shall to  the seep the seep \n",
      "o the rowe \n",
      " the eane the soul  tnd the soul aour sarder toree the e  the word thethe shall tour be the sord to me to tour  to the world\n",
      "o the wordh to the soall to the shall be the couse \n",
      "  the comes \n",
      "   And Ior the  the  shall be tour tour end the  toe toee tord the soall the sand the eeor the  ea tor the eaather to the   re the e the sour and tor tour soere the  eat  to tou shall the seep the  eree to tour bear tou stand thethe shall to    the son  the e toe   the e the parting the son to  the soue the  the word \n",
      "   The some to man the seall to me thethe son  our barter the sore to   the r starn to the e the sharle \n",
      "  AAnd to tor the see the e the sare  th the sane  the stand the  the sheat tou sould the see  to the santion the come nt to toue the soul the seep the word th the r the some thete to moe the sord tou  the  the son the  sourd the sone the seep the sord \n",
      " hat  to the e the shall the see   the wing \n",
      "o tour have the paint to the sart tor the sone the e to  the stand the e rou  the e shell the soae the some to the e the sare to moe theee the soeat the stall to   thet the soue th tou shall the court the  the wanger thet the sare to the canse to the stall \n",
      "    The sand the saall the seart \n",
      "    The  and tou sou are the word oo  to man  o the sare to   tour soue the send to toe toe toe   the soall the srown \n",
      "    The sore  the  tou sae the seeveng ou to the sport tou so be the  one the sarth of the seep \n",
      "   ARINIU. Ihe e ou shall the sone the  tourh the r the son to the  tore \n",
      "    The sing the say to the e  o       the e to  the seep the saall the   tou the e the sare   to the saow the sheet tou shall be the sart the  the  are \n",
      " o tor the searl the wordd  the wart of the e the soall the soe t the shall the  the strrn the seare the soul to the sorerent    the e  the soele  the soul the e ere to  the  the word \n",
      "   TThe  and the shall tou  ord  the saye the e t the word to  the sart the seae tou  th the see the  the word the  shall the saarl \n",
      "    Thet th  sare the stre the ear  o the sterer  to  to tou  shall the sore to  the sane \n",
      " toue \n",
      "\n",
      "SIENG RIRDICH:\n",
      "And the shall shall the sore to e th toe tord  thethe wordh    AThe was  the seae the  the r tou the wordh the soeak \n",
      "    The san the shall the r the ware to the stand the tare  the seart th te tor the  and the sane to   the r toue the staen the e th the saye to the saae \n",
      "\n",
      "GOOUC S:\n",
      "Ay, and tou  the seall the saae the  the shall be the seep the say the seae to mo the e the  to the e toe the sarter tnd the   and the seall the soall to mou toe  the soall to me lord \n",
      "   ATo the  the cane  th  tou shel the sore the e is toue to man      To the shall  ou shall the see the sore t the soall the ware the word \n",
      "   ARENTER. Whll the saye  the word to the  the  the e to   tou  tour the some  the say the soul  to the  the  the  tord \n",
      "    The sane  the sarth the say the sing \n",
      "o toe the sand thethe say \n",
      "    The e the staang to the wang the saall  the seat the seart  the e to the sord of the eard of the seepe\n",
      "   ATo the come tour thet toe torr  o   the saae  th the say \n",
      "   ARD IIN. The san the word the care to the seever the seare tnd tour sone the e the soun to the strong \n",
      "\n",
      "SUCENTER:\n",
      "The e or tou the wore the word the word to the senver    to tour so shall shall the sarth   or the sane the stare to the server the e th  the sase the songer the sare \n",
      "ou will the  the staar  to the e the mare to tour th the serven the sane the e the words \n",
      "   AThe soare th tour to te tour tare\n"
     ]
    }
   ],
   "source": [
    "from hw3.answers import part1_generation_params\n",
    "\n",
    "start_seq, temperature = part1_generation_params()\n",
    "\n",
    "generated_sequence = charnn.generate_from_model(\n",
    "    model, start_seq, 10000, (char_to_idx,idx_to_char), T=temperature\n",
    ")\n",
    "\n",
    "print(generated_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lDJZd90M2ESk"
   },
   "source": [
    "## Questions\n",
    "<a id=part1_9></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5V8NjmN42ESk"
   },
   "source": [
    "**TODO** Answer the following questions. Write your answers in the appropriate variables in the module `hw3/answers.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T22:08:59.970191Z",
     "iopub.status.busy": "2021-12-26T22:08:59.969194Z",
     "iopub.status.idle": "2021-12-26T22:09:00.106826Z",
     "shell.execute_reply": "2021-12-26T22:09:00.106826Z"
    },
    "id": "6iAPW30J2ESk"
   },
   "outputs": [],
   "source": [
    "from cs3600.answers import display_answer\n",
    "import hw3.answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwMROJsx2ESk"
   },
   "source": [
    "### Question 1\n",
    "Why do we split the corpus into sequences instead of training on the whole text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T22:09:00.110815Z",
     "iopub.status.busy": "2021-12-26T22:09:00.109817Z",
     "iopub.status.idle": "2021-12-26T22:09:00.239473Z",
     "shell.execute_reply": "2021-12-26T22:09:00.239473Z"
    },
    "id": "Q1D2tAer2ESk"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "There are several reasons for using sequences and not the whole corpus while training. \n",
       "    1. Memory - loading the whole text and running forward and backprop on it whole require a very large amount of memory, which is unfeasible on large corpura. \n",
       "    2. Learning the right context - with very large sequences or the entire corpus as input it will be hard for the model to learn contexts of the text. \n",
       "    3. Vanishing Gradients - Running on long sequences might cause the model to fall into the vanishing gradient problem.\n",
       "     \n",
       "Of course, splitting the corpus into very small sequences (e.g. 1) might lead to overfitting.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(hw3.answers.part1_q1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZ4ZL2YM2ESl"
   },
   "source": [
    "### Question 2\n",
    "How is it possible that the generated text clearly shows memory longer than the sequence length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T22:09:00.243462Z",
     "iopub.status.busy": "2021-12-26T22:09:00.242464Z",
     "iopub.status.idle": "2021-12-26T22:09:00.365136Z",
     "shell.execute_reply": "2021-12-26T22:09:00.364139Z"
    },
    "id": "usRwfx8u2ESl"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "** This is due to the passing the hidden layer across sequences, as the model takes in a new sequence input \n",
       "it also takes in the hidden states of the previous input, thus the model's 'memory' could showen further \n",
       "then the sequence length. **\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(hw3.answers.part1_q2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuuJ2Gdk2ESl"
   },
   "source": [
    "### Question 3\n",
    "Why are we not shuffling the order of batches when training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T22:09:00.368128Z",
     "iopub.status.busy": "2021-12-26T22:09:00.368128Z",
     "iopub.status.idle": "2021-12-26T22:09:00.488827Z",
     "shell.execute_reply": "2021-12-26T22:09:00.487809Z"
    },
    "id": "KgA8VGkM2ESl"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "We don't shuffle the order while training because the text order has an affect on the context and so we want that order to be maintained. Otherwise we will loss the knowledge being passed by the hidden state.  \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(hw3.answers.part1_q3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ri0GWA5Z2ESl"
   },
   "source": [
    "### Question 4\n",
    "1. Why do we lower the temperature for sampling (compared to the default of $1.0$)?\n",
    "2. What happens when the temperature is very high and why?\n",
    "3. What happens when the temperature is very low and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-26T22:09:00.491798Z",
     "iopub.status.busy": "2021-12-26T22:09:00.491798Z",
     "iopub.status.idle": "2021-12-26T22:09:00.614468Z",
     "shell.execute_reply": "2021-12-26T22:09:00.613471Z"
    },
    "id": "sBe30h762ESl"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "1. The tempature hyperparameter affects the probabilities of the chars from the softmax. During training we want the model to expriement by choosing different chars and only the most proabable ones. however, when generating text we want lower 'mistakes' and to stick to probabilities learned by the model. And so and by lowering the tempature we are giving a higher probability to models chosen chars.  \n",
       "2. When the tempature is very high there tends to be more mistakes but there is also more diversity. A very high tempature will cause the model to randomly select words from the corpus (e.g. uniform probability between words). \n",
       "3. When we lower the tempature we our increasing our models's confidence but also causing it to be more conservative in it's samples because it sticks to the more probable possiblities. When the tempature approaches 0 it is most likely to get stuck in an infinite loop, this happens because your model becomes very confident and doesn't diversify the text. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_answer(hw3.answers.part1_q4)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Part1_Sequence.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyCharm (DL_class)",
   "language": "python",
   "name": "pycharm-b9ffc24f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}